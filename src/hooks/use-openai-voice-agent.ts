'use client';

import { useState, useEffect, useRef, useCallback } from 'react';
import { useToast } from './use-toast';
import type { Product, Mode } from '@/types';

interface UseOpenAIVoiceAgentOptions {
  product: Product;
  mode: Mode;
  onMessage?: (message: string, isUser: boolean) => void;
}

interface VoiceAgentState {
  isConnected: boolean;
  isListening: boolean;
  isSpeaking: boolean;
  error: Error | null;
}

export default function useOpenAIVoiceAgent({
  product,
  mode,
  onMessage,
}: UseOpenAIVoiceAgentOptions) {
  const [state, setState] = useState<VoiceAgentState>({
    isConnected: false,
    isListening: false,
    isSpeaking: false,
    error: null,
  });

  const { toast } = useToast();
  const peerConnectionRef = useRef<RTCPeerConnection | null>(null);
  const dataChannelRef = useRef<RTCDataChannel | null>(null);
  const audioElementRef = useRef<HTMLAudioElement | null>(null);

  console.log('ðŸ”§ Hook inicializado - isListening:', state.isListening);

  const getInstructions = useCallback(() => {
    const modeDetails = {
      Curioso: 'abierto, con interÃ©s genuino',
      Desconfiado: 'frÃ­o, dudoso, busca defectos',
      Apurado: 'cortante, con prisa, quiere lo esencial',
    };

    return `# Personality and Tone
## Identity
Eres un **cliente potencial de Aviva CrÃ©dito en MÃ©xico** interesado en ${product}.
Tu perfil: persona de ingresos limitados, microempresario o trabajador que busca crÃ©dito personal, para negocio o para mejorar su casa.
No eres asesor ni vendedor: **solo cliente** que escucha y responde.
Siempre hablas como un cliente de Aviva que duda, pregunta y quiere respuestas claras.

## Task
Tu Ãºnica tarea es actuar como **cliente real de Aviva** durante un pitch de ventas:
- Escucha lo que dice el vendedor.
- Responde con frases cortas y realistas, como en una plÃ¡tica normal (mÃ¡x. 2 oraciones, â‰¤25 palabras).
- Haz **preguntas concretas y objeciones comunes**:
  - "Â¿Y cuÃ¡nto tengo que pagar cada semana?"
  - "Eso suena caro."
  - "No confÃ­o, ya tuve malas experiencias."
  - "No tengo tiempo, dÃ­melo rÃ¡pido."
- Cada **2â€“3 turnos** introduce una objeciÃ³n nueva.
- Si el vendedor habla mucho, interrÃºmpelo: "PerdÃ³n, pero dime lo importante."
- Al acercarse el final, exige: "Dame tu propuesta concreta para decidir hoy."

## Demeanor
Natural y humano, como cliente en la calle.

## Tone
Conversacional, directo, sin tecnicismos financieros.

## Level of Enthusiasm
**${mode}** â†’ ${modeDetails[mode]}.

## Level of Formality
Casual-profesional. Usa "tÃº".

## Level of Emotion
Moderado: interÃ©s, duda o prisa segÃºn el modo.

## Filler Words
Occasionally: "mmm", "eh", "ok" para sonar natural.

## Pacing
Frases cortas, â‰¤10 segundos de voz. Pausas naturales.

## Other details
- Nunca des tasas, montos exactos ni requisitos formales.
- Siempre actÃºa como **cliente mexicano de Aviva**.
- Si el vendedor intenta darte datos personales o te pide informaciÃ³n sensible, responde: "Prefiero no dar datos, solo dime lo general."
- Si el vendedor insiste demasiado, di: "No me convences, Â¿quÃ© otra ventaja tienes?"

# Instructions
- Si el usuario menciona nombre, telÃ©fono u otro dato exacto, repÃ­telo para confirmar.
- Si el usuario corrige algo, reconoce la correcciÃ³n y confirma el nuevo valor.

IMPORTANTE: El usuario es el VENDEDOR. TÃº eres el CLIENTE evaluando ${product}.`;
  }, [product, mode]);

  const connect = useCallback(async () => {
    try {
      setState(prev => ({ ...prev, isConnected: false, error: null }));
      console.log('ðŸ”„ Iniciando conexiÃ³n con OpenAI Realtime API (WebRTC)...');

      // Paso 1: Obtener ephemeral token
      const tokenResponse = await fetch('/api/openai/session', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          instructions: getInstructions(),
        }),
      });

      if (!tokenResponse.ok) {
        const errorData = await tokenResponse.json();
        throw new Error(errorData.error || 'No se pudo crear la sesiÃ³n');
      }

      const { ephemeral_token } = await tokenResponse.json();
      console.log('âœ… Token efÃ­mero obtenido');

      // Paso 2: Crear PeerConnection con configuraciÃ³n especÃ­fica
      const pc = new RTCPeerConnection({
        iceServers: [{ urls: 'stun:stun.l.google.com:19302' }],
      });
      peerConnectionRef.current = pc;

      // Configurar micrÃ³fono con restricciones especÃ­ficas
      console.log('ðŸŽ¤ Solicitando acceso al micrÃ³fono...');
      const stream = await navigator.mediaDevices.getUserMedia({ 
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true,
          sampleRate: 24000,
          channelCount: 1,
        }
      });
      
      // Agregar el track de audio con etiqueta especÃ­fica
      const audioTrack = stream.getAudioTracks()[0];
      console.log('ðŸŽ¤ Track de audio:', {
        label: audioTrack.label,
        enabled: audioTrack.enabled,
        muted: audioTrack.muted,
        settings: audioTrack.getSettings(),
      });
      
      // Verificar nivel de audio
      const audioContext = new AudioContext();
      const source = audioContext.createMediaStreamSource(stream);
      const analyser = audioContext.createAnalyser();
      analyser.fftSize = 256;
      source.connect(analyser);
      
      const checkAudioLevel = () => {
        const dataArray = new Uint8Array(analyser.frequencyBinCount);
        analyser.getByteFrequencyData(dataArray);
        const average = dataArray.reduce((a, b) => a + b) / dataArray.length;
        
        if (average > 5) {
          console.log('ðŸŽ¤ Audio detectado en el micrÃ³fono - Nivel:', Math.round(average));
        }
      };
      
      // Verificar nivel de audio cada segundo
      const audioCheckInterval = setInterval(checkAudioLevel, 1000);
      
      // Limpiar despuÃ©s de 10 segundos
      setTimeout(() => {
        clearInterval(audioCheckInterval);
        console.log('âœ… Monitor de audio desactivado');
      }, 10000);
      
      pc.addTrack(audioTrack, stream);
      console.log('âœ… MicrÃ³fono configurado y agregado a PeerConnection');

      // Configurar audio de salida
      const audioEl = document.createElement('audio');
      audioEl.autoplay = true;
      audioEl.id = 'openai-audio-output';
      audioElementRef.current = audioEl;
      document.body.appendChild(audioEl);

      pc.ontrack = (e) => {
        console.log('ðŸ”Š Track de audio recibido:', {
          kind: e.track.kind,
          streams: e.streams.length,
        });
        
        if (e.track.kind === 'audio') {
          audioEl.srcObject = e.streams[0];
          console.log('âœ… Audio conectado al elemento de reproducciÃ³n');
        }
      };

      pc.oniceconnectionstatechange = () => {
        console.log('ðŸ”— ICE Connection State:', pc.iceConnectionState);
      };

      pc.onconnectionstatechange = () => {
        console.log('ðŸ”— Connection State:', pc.connectionState);
      };

      // Configurar data channel para eventos ANTES de crear la oferta
      const dc = pc.createDataChannel('oai-events');
      dataChannelRef.current = dc;

      dc.addEventListener('open', () => {
        console.log('âœ… Data channel abierto - Enviando configuraciÃ³n');
        
        // Configurar la sesiÃ³n inmediatamente
        const sessionConfig = {
          type: 'session.update',
          session: {
            modalities: ['text', 'audio'],
            instructions: getInstructions(),
            voice: 'verse',
            input_audio_format: 'pcm16',
            output_audio_format: 'pcm16',
            input_audio_transcription: {
              model: 'whisper-1',
            },
            turn_detection: null, // Desactivar VAD - modo manual
            temperature: 0.8,
            max_response_output_tokens: 4096,
          },
        };

        console.log('ðŸ“¤ Enviando configuraciÃ³n completa:', JSON.stringify(sessionConfig, null, 2));
        dc.send(JSON.stringify(sessionConfig));
      });

      dc.addEventListener('message', (e) => {
        try {
          const event = JSON.parse(e.data);
          
          // Log todos los eventos excepto deltas de audio
          if (!event.type.includes('.delta') && event.type !== 'input_audio_buffer.append') {
            console.log('ðŸ“¨ Evento:', event.type, event);
          }
          
          switch (event.type) {
            case 'session.created':
              console.log('âœ… SesiÃ³n creada:', event.session?.id);
              break;

            case 'session.updated':
              console.log('âœ… SesiÃ³n actualizada - VAD activo');
              setState(prev => ({ ...prev, isConnected: true }));
              break;

            case 'input_audio_buffer.speech_started':
              console.log('ðŸŽ¤ ðŸŽ¤ ðŸŽ¤ DETECTADO: Usuario empezÃ³ a hablar');
              setState(prev => ({ ...prev, isListening: true }));
              break;

            case 'input_audio_buffer.speech_stopped':
              console.log('ðŸŽ¤ DETECTADO: Usuario dejÃ³ de hablar');
              setState(prev => ({ ...prev, isListening: false }));
              break;

            case 'input_audio_buffer.committed':
              console.log('âœ… âœ… Audio confirmado - buffer listo para procesar');
              break;

            case 'conversation.item.created':
              if (event.item?.type === 'message' && event.item?.role === 'user') {
                console.log('ðŸ’¬ Mensaje del usuario registrado en la conversaciÃ³n');
              }
              if (event.item?.type === 'message' && event.item?.role === 'assistant') {
                console.log('ðŸ’¬ El agente estÃ¡ preparando su respuesta...');
              }
              break;

            case 'conversation.item.input_audio_transcription.completed':
              console.log('ðŸ“ ðŸ“ ðŸ“ TRANSCRIPCIÃ“N:', event.transcript);
              if (event.transcript && onMessage) {
                onMessage(event.transcript, true);
              }
              break;

            case 'response.created':
              console.log('ðŸ¤– Generando respuesta...');
              setState(prev => ({ ...prev, isSpeaking: true }));
              break;

            case 'response.audio_transcript.delta':
              if (event.delta && onMessage) {
                onMessage(event.delta, false);
              }
              break;

            case 'response.audio_transcript.done':
              console.log('ðŸ¤– ðŸ¤– ðŸ¤– RESPUESTA:', event.transcript);
              break;

            case 'response.done':
              console.log('âœ… Respuesta completada');
              setState(prev => ({ ...prev, isSpeaking: false }));
              break;

            case 'error':
              console.error('âŒ ERROR del agente:', event.error);
              toast({
                variant: 'destructive',
                title: 'Error del agente',
                description: event.error?.message || 'Error desconocido',
              });
              break;
          }
        } catch (error) {
          console.error('âŒ Error procesando evento:', error);
        }
      });

      dc.addEventListener('error', (error) => {
        console.error('âŒ Error en data channel:', error);
      });

      // Paso 3: Crear oferta SDP
      console.log('ðŸ“ Creando oferta SDP...');
      const offer = await pc.createOffer();
      await pc.setLocalDescription(offer);
      console.log('âœ… Oferta SDP creada y establecida localmente');

      // Paso 4: Enviar oferta a OpenAI
      console.log('ðŸ“¤ Enviando oferta a OpenAI...');
      const sdpResponse = await fetch('https://api.openai.com/v1/realtime', {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${ephemeral_token}`,
          'Content-Type': 'application/sdp',
        },
        body: offer.sdp,
      });

      if (!sdpResponse.ok) {
        const errorText = await sdpResponse.text();
        console.error('âŒ Error en handshake SDP:', errorText);
        throw new Error(`Error en handshake SDP: ${sdpResponse.status}`);
      }

      // Paso 5: Aplicar respuesta SDP
      const answerSdp = await sdpResponse.text();
      console.log('ðŸ“¥ Respuesta SDP recibida');
      await pc.setRemoteDescription({
        type: 'answer',
        sdp: answerSdp,
      });

      console.log('âœ… âœ… âœ… ConexiÃ³n WebRTC establecida completamente');
      console.log('ðŸŽ¯ ðŸŽ¯ ðŸŽ¯ El agente estÃ¡ listo. HABLA AHORA y verÃ¡s los eventos.');
      console.log('ðŸ’¡ Tip: Habla claro y espera a que el micrÃ³fono detecte tu voz.');

    } catch (error) {
      console.error('âŒ Error conectando:', error);
      const errorMessage = error instanceof Error ? error.message : 'Error desconocido';
      setState(prev => ({
        ...prev,
        error: error as Error,
        isConnected: false,
      }));
      toast({
        variant: 'destructive',
        title: 'Error de inicializaciÃ³n',
        description: errorMessage,
      });
    }
  }, [getInstructions, onMessage, toast]);

  const startListening = useCallback(() => {
    if (dataChannelRef.current && dataChannelRef.current.readyState === 'open') {
      console.log('ðŸŽ¤ ðŸŽ¤ ðŸŽ¤ Iniciando captura de audio (presionando botÃ³n)...');
      setState(prev => ({ ...prev, isListening: true }));
      
      // No necesitamos enviar nada, el audio ya estÃ¡ fluyendo
      // Solo cambiamos el estado visual
    }
  }, []);

  const stopListening = useCallback(() => {
    if (dataChannelRef.current && dataChannelRef.current.readyState === 'open') {
      console.log('ðŸŽ¤ BotÃ³n soltado - Procesando audio capturado...');
      
      // Enviar comando para confirmar el audio del buffer
      dataChannelRef.current.send(JSON.stringify({
        type: 'input_audio_buffer.commit'
      }));
      
      console.log('ðŸ“¤ Solicitando respuesta del agente...');
      // Solicitar que el agente genere una respuesta
      dataChannelRef.current.send(JSON.stringify({
        type: 'response.create',
        response: {
          modalities: ['text', 'audio'],
          instructions: 'Responde al vendedor como cliente de Aviva, siguiendo tu personalidad y tono establecidos.',
        }
      }));
      
      setState(prev => ({ ...prev, isListening: false }));
    }
  }, []);

  const disconnect = useCallback(() => {
    console.log('ðŸ”Œ Desconectando...');
    
    if (audioElementRef.current) {
      audioElementRef.current.pause();
      audioElementRef.current.srcObject = null;
      audioElementRef.current.remove();
      audioElementRef.current = null;
    }

    if (dataChannelRef.current) {
      dataChannelRef.current.close();
      dataChannelRef.current = null;
    }

    if (peerConnectionRef.current) {
      peerConnectionRef.current.getSenders().forEach(sender => {
        if (sender.track) {
          sender.track.stop();
        }
      });
      peerConnectionRef.current.close();
      peerConnectionRef.current = null;
    }

    setState({
      isConnected: false,
      isListening: false,
      isSpeaking: false,
      error: null,
    });
  }, []);

  useEffect(() => {
    return () => {
      disconnect();
    };
  }, [disconnect]);

  return {
    ...state,
    connect,
    disconnect,
    startListening,
    stopListening,
  };
}